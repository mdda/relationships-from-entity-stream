
    
### ImageNet as an attention game
  2017 overview
    https://medium.com/towards-data-science/visual-attention-model-in-deep-learning-708813c2912c
    https://github.com/tianyu-tristan/Visual-Attention-Model

  Learning to combine foveal glimpses with a third-order Boltzmann machine
    https://papers.nips.cc/paper/4089-learning-to-combine-foveal-glimpses-with-a-third-order-boltzmann-machine
      Hugo Larochelle
      Geoffrey Hinton
    Issues:
      RBM approach
      Very retina-like field of view

  Learning where to Attend with Deep Architectures for Image Tracking
    https://arxiv.org/abs/1109.3737
    v. early 
    More like tracking
    Some Bayesian elements
      Misha Denil
      Loris Bazzani
      Hugo Larochelle
      Nando de Freitas
    Issues:
    
  Recurrent Models of Visual Attention  (DeepMind)
    https://arxiv.org/abs/1406.6247
      Volodymyr Mnih  *
      Nicolas Heess
      Alex Graves
      Koray Kavukcuoglu
    Blog Post:
      http://torch.ch/blog/2015/09/21/rmva.html
        This shows that each 8x8 pixel glimpse of 28x28 MNIST is individually tricky
    Issues:
      Glimpse size can be quite large 
        28x28 regular    MNIST sampled with 1 to 7 steps of 8x8 patches
        60x60 translated MNIST sampled with 1 to 3 steps of 8x8 patches, but covering up to 8*2*2=32x32
        p6 claims that a single 8x8 is insufficient to classify MNIST digits
        p9 shows that digits are clearly recognisable from 1 3-step glimpse patch
        
  On Learning Where To Look
    https://arxiv.org/abs/1405.5488
      Marc'Aurelio Ranzato (Google)
    Issues:
      1.  train N0 and N1
      2.  train N2, fixing N0 and N1
      3.  train N3, fixing N0, N1 and N2
      Help from : Hinton (xStudent?)
      
  Multiple Object Recognitions with Visual Attention (ICLR 2015, DeepMind)
    https://arxiv.org/pdf/1412.7755.pdf
      Jimmy Lei Ba
      Volodymyr Mnih  *
      Koray Kavukcuoglu  
    Blog Post:
      https://netsprawl.wordpress.com/2016/07/26/recurrent-attention/
      https://github.com/jrbtaylor/visual-attention
        Ultimately, I decided to abandon this track of research. 
        There may be some applications with extremely large images, 
          like microscopy, where a hard attention mechanism is necessary for now (until GPU memory can hold the images), 
        but otherwise the policy learning is so much slower than the convnet that the trade-off never works out.
    Issues:
      SVHN dataset (multiple digits recognised)
      Attention model took ~3 days on 'a' GPU
      Help from : Geoffrey Hinton, Nando de Freitas and Chris Summerfield
      
  Spatial Transformer Networks (DeepMind)
    Paper:
      https://arxiv.org/abs/1506.02025
      https://arxiv.org/pdf/1506.02025.pdf
        Max Jaderberg
        Karen Simonyan
        Andrew Zisserman
        Koray Kavukcuoglu
    Blog posts:
      https://kevinzakka.github.io/2017/01/10/stn-part1/
      https://kevinzakka.github.io/2017/01/18/stn-part2/
    Implementations:
      https://github.com/qassemoquab/stnbhwd (torch)
      https://github.com/kevinzakka/spatial_transformer_network  (TF)

  Attend, Infer, Repeat: Fast Scene Understanding with Generative Models
    https://arxiv.org/pdf/1603.08575.pdf
      S. M. Ali Eslami
      Nicolas Heess
      Theophane Weber
      Yuval Tassa
      David Szepesvari
      Koray Kavukcuoglu
      Geoffrey E. Hinton
    
      
  Show, Attend and Tell: Neural Image Caption Generation with Visual Attention
    https://arxiv.org/pdf/1502.03044.pdf
      Kelvin Xu
      Jimmy Lei Ba     (DeepMind?)
      Ryan Kiros
      Kyunghyun Cho
      Aaron Courville  *
      Ruslan Salakhutdinov *
      Richard S. Zemel
      Yoshua Bengio  *

   https://github.com/atulkum/paper_implementation  in TensorFlow
     Recurrent Models of Visual Attention https://arxiv.org/abs/1406.6247
     Multiple Object Recognition with Visual Attention https://arxiv.org/abs/1412.7755
     Show, Attend and Tell: Neural Image Caption Generation with Visual Attention https://arxiv.org/abs/1502.03044

   Recurrent Attention in Computer Vision
     https://netsprawl.wordpress.com/2016/07/26/recurrent-attention/
       https://github.com/jrbtaylor/visual-attention  (unfinished)
       
       
       
## Implementation :

Let's create a X/Y coordinate thing (2d)

The 'keys' for the image will be the first 10 of the 24 /concat with/ the coordinate thing
The 'values' for the image will be the second 14 of the 24 /concat with/ the coordinate thing

So the attention query vector (output) will need to be n_q=12 wide, and will retrieve a n_v=16 wide value

To get the attention weights, create a dot-product of the k&q numbers
*  Option 1 : Softmax these.  
*  Option 2 : use a temperature parameter on this vector Gumbel-SoftMax (Google ICLR-2017)
*  Option 3 : Harden the Gumbel-SoftMax outputs for testing (and training?)

https://arxiv.org/abs/1611.01144
  Categorical Reparameterization with Gumbel-Softmax
    This distribution has the essential property that it can be smoothly annealed into a categorical distribution. 
    Use this as an 'action' when temperature -> 0
See also : 
  https://casmls.github.io/general/2017/02/01/GumbelSoftmax.html

Since we don't know what 'language' will be constructed for the internal dialogue,
  we can't do teacher-forcing.  Will have to go the slower route...
  Similarly, using a dilated-CNN doesn't make sense, since part of the advantage is when doing forcing

So, the input to the LSTM thing at the bottom will be n_v-wide
  Initial input should be (say) all-zero
output stage to the LSTM thing at the top will be n_q-wide

Maximum length of objects to be processed is ~6.  
So make LSTM stage 8 long

#1 layer version
#  1 layer of LSTM.   n_v input size, hidden_size = (question_size+answer_size), n_q output size
#  - hidden state initialised with (question + trainable_vector(answer_size)) 
#     Output 'answer' corresponds to answer_size portion of hidden units

#2 layer version
#  2 layers of LSTM.   n_v input size, hidden_size_1 = question_size, hidden_size_2 = answer_size, n_q output size
#  - bottom one initialised with question - final output is ignored
#  - top one initialised with zero - and outputs answer
#     Output 'answer' should be hidden units of LSTM layer 2

Basic Query IO : 
  question=11d
  answer=10d

Two major layers : 
  First RNN : (The internal dialogue stage)
    inputs = 'vs(16d)' from image, hidden0=question[0:11]+0s[0:5]=16d, output = hidden*W = qs(12d) -> softmax -> weighted vs for next step

  Second RNN : (The interpreting the dialogue stage - in two stages)
    inputs = outputs from previous layer, hidden0=16d=question[0:11]+0s[0:5]=16d
    inputs = hiddens from previous layer, hidden0=0s[0:16]=16d, final answer=hidden_final[0:10]


Ideas:
  Add a 'zeroes' key/value to allow for non-attentive states
  Instead of later in hierarchy, add the positional information as extra input layer (so R,G,B,x,y)
  Teach each question in turn, building up the curriculum


