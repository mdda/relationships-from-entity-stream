{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See : http://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-custom-nn-modules\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, d = 32, 5  # d is size of input\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "t_long = torch.LongTensor\n",
    "t_float = torch.FloatTensor\n",
    "if use_cuda:\n",
    "    t_long = torch.cuda.LongTensor\n",
    "    t_float = torch.cuda.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_np = np.random.randint( d, size=(batch_size,1) )\n",
    "\n",
    "# Create Tensors to hold inputs and outputs, and wrap them in Variables\n",
    "x = Variable( torch.from_numpy(x_np).type( t_long ) )\n",
    "y = Variable( torch.from_numpy(x_np).type( t_long ).squeeze(), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gumbel(input):\n",
    "    noise = torch.rand(input.size())\n",
    "    eps = 1e-20\n",
    "    noise.add_(eps).log_().neg_()\n",
    "    noise.add_(eps).log_().neg_()\n",
    "    return Variable(noise).type(t_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectorNet(torch.nn.Module):\n",
    "  def __init__(self, D):\n",
    "    \"\"\"\n",
    "    In the constructor we instantiate nn modules and assign them as member variables.\n",
    "    \"\"\"\n",
    "    super(SelectorNet, self).__init__()\n",
    "    \n",
    "    self.input_space = torch.FloatTensor(batch_size, D).type(t_float) \n",
    "    self.linear = torch.nn.Linear(D, D)  # The weights in here should become the identity\n",
    "\n",
    "  def forward(self, i, fuzziness=1.0):\n",
    "    \"\"\"\n",
    "    In the forward function we accept a Variable of input data and we must return\n",
    "    a Variable of output data. We can use Modules defined in the constructor as\n",
    "    well as arbitrary operators on Variables.\n",
    "    \"\"\"\n",
    "    # Convert the input 'i' into a one-hot vector\n",
    "    self.input_space.zero_()\n",
    "    self.input_space.scatter_(1, i.data, 1.)\n",
    "    \n",
    "    x = Variable( self.input_space )\n",
    "    logits = self.linear(x)\n",
    "    \n",
    "    # These two seem to have problems with vanishing gradients (==0 on LHS)\n",
    "    #logits = logits.clamp(min=0)\n",
    "    #logits = torch.nn.ReLU()(logits)\n",
    "    \n",
    "    # Works fine\n",
    "    #logits = torch.nn.LeakyReLU()(logits)\n",
    "    \n",
    "    # Soft version\n",
    "    #action = logits\n",
    "    \n",
    "    # Hard op\n",
    "    #print( y_idx)\n",
    "    \n",
    "    # Do nothing else to get pure flow-through behaviour (Soft attention, though)\n",
    "    action = logits.clone()\n",
    "    \n",
    "    if False:  # This gives appropriate action(s) but no learning (?)\n",
    "        y_max, y_idx = torch.max(logits, 1, keepdim=True)\n",
    "        #print(action.size(), y_idx.size())\n",
    "        action[:,:] = 0.\n",
    "        action.scatter_(1, y_idx, 5.)\n",
    "        #print(y_idx,action)\n",
    "\n",
    "    if False:  # This gives appropriate action(s) and learns something, sometimes\n",
    "        y_max, y_idx = torch.max(logits, 1, keepdim=True)\n",
    "        action[:,:] = 0.\n",
    "        action.scatter_(1, y_idx, y_max)\n",
    "        #print(action)\n",
    "\n",
    "    if False:\n",
    "        gumbel = sample_gumbel(logits)\n",
    "\n",
    "        y_max, y_idx = torch.max(logits + gumbel*fuzziness, 1, keepdim=True)\n",
    "        action[:,:] = 0.\n",
    "        #action.scatter_(1, y_idx, y_max)\n",
    "        action.scatter_(1, y_idx, y_max+5.0)\n",
    "        #print(action)\n",
    "        \n",
    "    if True:\n",
    "        gumbel = sample_gumbel(logits)\n",
    "        action = action + gumbel*fuzziness\n",
    "        \n",
    "    return action  # This is a (batch_size, d) matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SelectorNet(d)\n",
    "if use_cuda: model = model.cuda()\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the nn modules \n",
    "# which are members of the model.\n",
    "\n",
    "criterion = torch.nn.NLLLoss(size_average=True)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=1e-4) # \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-1) # \n",
    "\n",
    "for t in range(100):\n",
    "  # Forward pass: Compute predicted y by passing x to the model\n",
    "  y_pred_train = model(x)\n",
    "  y_pred_train_idx = y_pred_train.data.max(1)[1]\n",
    "  correct_train = y_pred_train_idx.eq(y.data).cpu().sum()    \n",
    "    \n",
    "  y_pred_test  = model(x, fuzziness=0.0)\n",
    "  y_pred_test_idx = y_pred_test.data.max(1)[1]\n",
    "  correct_test = y_pred_test_idx.eq(y.data).cpu().sum()\n",
    "    \n",
    "  loss = criterion(y_pred_train, y)\n",
    "  if (t+1) % 1 ==0 :\n",
    "      print(\"%4d %+6.2f %6.2f%% %6.2f%%\" % \n",
    "            (t+1, loss.data[0], correct_train*100./batch_size, correct_test*100./batch_size,))\n",
    "\n",
    "  # Zero gradients, perform a backward pass, and update the weights.\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "print(model.linear.weight.data,)# model.linear.bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = Variable(torch.rand(5, 3), requires_grad=True)\n",
    "a = a.clone() # Otherwise we change inplace a leaf Variable\n",
    "print(a)\n",
    "\n",
    "ind = Variable(torch.LongTensor([3]))\n",
    "a.index_fill_(0, ind, 0)\n",
    "\n",
    "print(a)\n",
    "\n",
    "a[1, :] = 0\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
