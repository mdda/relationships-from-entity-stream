{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "np.set_printoptions(formatter={'float_kind':(lambda x: \"%6.2f\" % x)})\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import argparse\n",
    "#parser = argparse.ArgumentParser(\n",
    "#            description='PyTorch Relations-from-Stream sort-of-CLVR Example')\n",
    "#args = parser.parse_args()\n",
    "\n",
    "from attrdict import AttrDict\n",
    "args = AttrDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.batch_size = 32\n",
    "args.cuda = torch.cuda.is_available()\n",
    "args.lr   = 0.0001\n",
    "args.seed = 5\n",
    "args.process_coords=False\n",
    "args.debug = True\n",
    "args.rnn_hidden_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in a batch of images to test dimensions, etc...\n",
    "bs = args.batch_size\n",
    "\n",
    "input_img = torch.FloatTensor(bs, 3, 75, 75)\n",
    "input_qst = torch.FloatTensor(bs, 11)\n",
    "label = torch.LongTensor(bs)\n",
    "\n",
    "if args.cuda:\n",
    "    input_img = input_img.cuda()\n",
    "    input_qst = input_qst.cuda()\n",
    "    label = label.cuda()\n",
    "    \n",
    "input_img = Variable(input_img)\n",
    "input_qst = Variable(input_qst)\n",
    "label = Variable(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dirs = './data'\n",
    "\n",
    "filename = os.path.join(data_dirs, 'sort-of-clevr++.pickle')\n",
    "with open(filename, 'rb') as f:\n",
    "  train_datasets, test_datasets = pickle.load(f)\n",
    "\n",
    "def cvt_data_axis(data):\n",
    "    img = [e[0] for e in data]\n",
    "    qst = [e[1] for e in data]\n",
    "    ans = [e[2] for e in data]\n",
    "    return (img,qst,ans)\n",
    "\n",
    "rel_train, norel_train = [], []\n",
    "for img, relations, norelations in train_datasets:\n",
    "    img = np.swapaxes(img,0,2)\n",
    "    for qst,ans in zip(relations[0], relations[1]):\n",
    "        rel_train.append((img,qst,ans))\n",
    "    for qst,ans in zip(norelations[0], norelations[1]):\n",
    "        norel_train.append((img,qst,ans))\n",
    "\n",
    "rel   = cvt_data_axis(rel_train)\n",
    "norel = cvt_data_axis(norel_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_data(data, i):\n",
    "    img = torch.from_numpy(np.asarray(data[0][bs*i:bs*(i+1)]))\n",
    "    qst = torch.from_numpy(np.asarray(data[1][bs*i:bs*(i+1)]))\n",
    "    ans = torch.from_numpy(np.asarray(data[2][bs*i:bs*(i+1)]))\n",
    "\n",
    "    input_img.data.resize_(img.size()).copy_(img)\n",
    "    input_qst.data.resize_(qst.size()).copy_(qst)\n",
    "    label.data.resize_(ans.size()).copy_(ans)\n",
    "\n",
    "tensor_data(norel, 0)\n",
    "#tensor_data(rel, 0)    # Loads batch 0 into input_img, input_qst amd label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_pytorch(p): \n",
    "    #print(type(p))\n",
    "    if isinstance(p, Variable):\n",
    "        return p.cpu().data.numpy()\n",
    "    return p\n",
    "\n",
    "def show_image(img):\n",
    "    im = np.swapaxes( from_pytorch(img), 0,2)  # Undo the np->pytorch swap\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(im[:,:,::-1], interpolation='nearest')  # BGR-> RGB\n",
    "    #plt.axis('off')\n",
    "\n",
    "colors = 'red green blue orange gray yellow'.split()\n",
    "def describe_question(q):\n",
    "    question = list(from_pytorch(q))\n",
    "    color_arr = [colors[i] for i in range(0,6) if question[i]==1.]\n",
    "    #query = colors[question[0:6].index(1)]\n",
    "    query = '[%s] ' % ','.join(color_arr)\n",
    "\n",
    "    if question[6]==1 and question[7]==0 :  # NonRel Questions\n",
    "        if question[8] == 1:  query += 'shape?'\n",
    "        if question[9] == 1:  query += 'left?'\n",
    "        if question[10] == 1: query += 'up?'\n",
    "            \n",
    "    if question[6]==0 and question[7]==1:  # BiRel questions\n",
    "        if question[8] == 1:  query += 'closest shape?'\n",
    "        if question[9] == 1:  query += 'furthest shape?'\n",
    "        if question[10] == 1: query += 'shape count?'\n",
    "\n",
    "    if question[6]==1 and question[7]==1:  # Tricky questions\n",
    "        if question[8] == 1:  query += 'big area triangle?'\n",
    "        if question[9] == 1:  query += 'clockwise?'\n",
    "        if question[10] == 1: query += 'most isolated shape?'\n",
    "    return query\n",
    "\n",
    "answer_sheet = ['yes', 'no', 'rectangle', 'circle', '1', '2', '3', '4', '5', '6']\n",
    "def describe_answer(a):\n",
    "    answer = from_pytorch(a)[0]\n",
    "    return answer_sheet[answer]\n",
    "\n",
    "def to_array(v): return v.cpu().data.numpy()\n",
    "\n",
    "def heat_map(d2, Xmap_to_zero=3.0):  # exp() factor below max that gets mapped to 0\n",
    "    d2_np = d2[0:25].view(5,5).t().cpu().data.numpy()\n",
    "    #d2_max = np.max(d2_np)\n",
    "    #d2_rescale = np.maximum(0., 1.0+(d2_np-d2_max)/map_to_zero)\n",
    "    print(d2_np)\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(np.exp(d2_np), interpolation='nearest', cmap='gray')  # copper, binary\n",
    "\n",
    "def show_example(i):\n",
    "    print( describe_question( input_qst[i] ), describe_answer( label[i] ) )\n",
    "    show_image(input_img[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_choice = 22\n",
    "show_example(example_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from model import RN, CNN_MLP, RFS\n",
    "import model\n",
    "importlib.reload(model)\n",
    "\n",
    "args.process_coords=False\n",
    "\n",
    "m = model.RFS(args)\n",
    "if args.cuda:\n",
    "    m.cuda()\n",
    "m.train();\n",
    "\n",
    "#accuracy_rel = m.train_(input_img, input_qst, label)\n",
    "#accuracy_norel = m.train_(input_img, input_qst, label)\n",
    "\n",
    "# Load a snapshot\n",
    "#m.load_state_dict( torch.load('model/RFS_2item-span-again-seed10_050.pth') )\n",
    "m.load_state_dict( torch.load('model/RFS_2item-span-again-seed10_050.pth', map_location=lambda storage, loc: storage) )\n",
    "\n",
    "m.optimizer.zero_grad()\n",
    "output = m(input_img, input_qst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( to_array(m.ans_logits[example_choice]), \n",
    "       ' answer = %s' % answer_sheet[ np.argmax(to_array(m.ans_logits[example_choice])) ] )\n",
    "for i in range(len(m.ent_similarities)):\n",
    "    heat_map(m.ent_similarities[i][example_choice])\n",
    "#m.ent_similarities[example_choice].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros(8, 2)\n",
    "b = torch.ones(8, 7)\n",
    "\n",
    "c = torch.cat( (a,b), 1)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.rand( (1,5) )\n",
    "p\n",
    "p.expand( (6,5) )\n",
    "p.expand( (6,5) ) + torch.rand( (6,5) )\n",
    "#p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.from_numpy( np.array([[ 1.,2.,3.], [6.,1.,4. ] ], dtype=np.float32) )\n",
    "b = Variable(b)\n",
    "b\n",
    "torch.nn.functional.softmax(b)    # This is the expected one (probs add up to 1 along rows)\n",
    "#torch.nn.functional.log_softmax(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.sample_gumbel(b) \n",
    "model.gumbel_softmax_sample(b, temperature=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the sort-of-CLEVR generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "area =  -346.5\n",
      "[red,blue,yellow] big area triangle?\n",
      "no\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWsAAAFpCAYAAABajglzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efdfa8c90f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sort_of_clevr_generator\n",
    "importlib.reload(sort_of_clevr_generator);\n",
    "\n",
    "(img, norelations, birelations, trirelations) = sort_of_clevr_generator.build_dataset(1)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(img[:,:,::-1], interpolation='nearest')  # BGR-> RGB\n",
    "\n",
    "example_choice = 0\n",
    "r = trirelations #, birelations, norelations\n",
    "print( describe_question(np.array(r[0][example_choice])) )\n",
    "print( describe_answer(np.array([r[1][example_choice]])) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#isinstance(np.array([]), np.ndarray)\n",
    "#np.cross( np.array( [-10,-10] ), np.array( [10,10] ) )\n",
    "np.linalg.norm( np.array( [10,0] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
